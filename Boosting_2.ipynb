{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is Gradient Boosting Regression?\n",
        "\n",
        "\n",
        "ANS- Gradient Boosting Regression is a machine learning technique used for regression tasks, aiming to predict continuous numeric values. It's an ensemble learning method that combines the predictive power of multiple weak learners, often decision trees, to create a stronger predictive model.\n",
        "\n",
        "Here's how it generally works:\n",
        "\n",
        "1. **Weak Learners (Decision Trees):** The base models, typically decision trees, are utilized as weak learners. These trees are constructed sequentially, each one correcting the errors of its predecessor.\n",
        "\n",
        "2. **Sequential Building:** The algorithm begins by creating a simple model to make predictions. Subsequent models are then built to correct the errors made by the previous models. This process continues iteratively.\n",
        "\n",
        "3. **Gradient Descent:** Gradient boosting works by minimizing the loss function of the model. In each iteration, it calculates the gradient of the loss function and fits the new model to the residuals or errors made by the previous model.\n",
        "\n",
        "4. **Combining Predictions:** The final prediction is a combination of the predictions made by all the weak learners, typically by summing them up.\n",
        "\n",
        "Gradient Boosting Regression, such as Gradient Boosting Machines (GBM), XGBoost, LightGBM, and CatBoost, has become popular due to its robustness and ability to handle complex non-linear relationships in data. It's often used in various domains like finance, healthcare, and natural language processing due to its accuracy and flexibility in handling different types of data."
      ],
      "metadata": {
        "id": "NnmR2kDgdot1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared.\n"
      ],
      "metadata": {
        "id": "7mdn1HJgdwkb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3QbParFbVOB",
        "outputId": "0af03915-02b7-4d2e-84cc-17eb96e08d8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 48.12342546550239\n",
            "R-squared: 0.9680846696905234\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class GradientBoostingRegressor:\n",
        "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_depth = max_depth\n",
        "        self.models = []\n",
        "        self.init_val = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.init_val = np.mean(y)\n",
        "        residuals = y - self.init_val\n",
        "        prediction = np.full_like(y, self.init_val)\n",
        "\n",
        "        for _ in range(self.n_estimators):\n",
        "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
        "            tree.fit(X, residuals)\n",
        "            self.models.append(tree)\n",
        "            update = tree.predict(X)\n",
        "            prediction += self.learning_rate * update\n",
        "            residuals = y - prediction\n",
        "\n",
        "    def predict(self, X):\n",
        "        y_pred = np.full(len(X), self.init_val)\n",
        "        for model in self.models:\n",
        "            y_pred += self.learning_rate * model.predict(X)\n",
        "        return y_pred\n",
        "\n",
        "# Creating a simple dataset\n",
        "X, y = make_regression(n_samples=100, n_features=1, noise=5, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Importing DecisionTreeRegressor for simplicity\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "# Training the Gradient Boosting Regressor\n",
        "gb_regressor = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
        "gb_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = gb_regressor.predict(X_test)\n",
        "\n",
        "# Evaluating the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Mean Squared Error: {mse}\")\n",
        "print(f\"R-squared: {r2}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to\n",
        "optimise the performance of the model. Use grid search or random search to find the best hyperparameters"
      ],
      "metadata": {
        "id": "a1QcaYKgeH7G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# grid search and have the best hyperparameters in best_params\n",
        "\n",
        "from sklearn.base import BaseEstimator, RegressorMixin\n",
        "\n",
        "class CustomGradientBoostingRegressor(BaseEstimator, RegressorMixin):\n",
        "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_depth = max_depth\n",
        "        self.models = []\n",
        "        self.init_val = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Your fitting logic here\n",
        "        pass\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Your prediction logic here\n",
        "        pass\n",
        "\n",
        "    def get_params(self, deep=True):\n",
        "        return {\n",
        "            'n_estimators': self.n_estimators,\n",
        "            'learning_rate': self.learning_rate,\n",
        "            'max_depth': self.max_depth\n",
        "        }\n",
        "\n",
        "    def set_params(self, **params):\n",
        "        if 'n_estimators' in params:\n",
        "            self.n_estimators = params['n_estimators']\n",
        "        if 'learning_rate' in params:\n",
        "            self.learning_rate = params['learning_rate']\n",
        "        if 'max_depth' in params:\n",
        "            self.max_depth = params['max_depth']\n",
        "        return self\n"
      ],
      "metadata": {
        "id": "JiUgRNgref28"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Creating a simple dataset\n",
        "X, y = make_regression(n_samples=100, n_features=1, noise=5, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Implement your CustomGradientBoostingRegressor here (as defined in the previous conversation)\n",
        "\n",
        "# Create the wrapper for your custom gradient boosting regressor\n",
        "custom_gb_regressor = CustomGradientBoostingRegressor()\n",
        "\n",
        "# Perform Grid Search (assuming you've already done this and obtained the best_params)\n",
        "best_params = {'n_estimators': 100, 'learning_rate': 0.1, 'max_depth': 3}  # Replace with your best params\n",
        "\n",
        "# Reinitialize the model with the best parameters\n",
        "best_custom_gb_regressor = CustomGradientBoostingRegressor(**best_params)\n",
        "\n",
        "# Fit the model with the best parameters\n",
        "best_custom_gb_regressor.fit(X_train, y_train)\n",
        "# Predictions using the best model\n",
        "y_pred_best = best_custom_gb_regressor.predict(X_test)\n",
        "\n",
        "# Check for None values in y_test and y_pred_best\n",
        "print(y_test is None)\n",
        "print(y_pred_best is None)\n",
        "\n",
        "# Check the data types directly\n",
        "print(type(y_test))\n",
        "print(type(y_pred_best))\n",
        "\n",
        "# Check shapes and types\n",
        "print(y_test.shape if y_test is not None else \"y_test is None\")\n",
        "print(y_pred_best.shape if y_pred_best is not None else \"y_pred_best is None\")\n",
        "\n",
        "# Ensure y_test and y_pred_best are not None before computing mean squared error\n",
        "if y_test is not None and y_pred_best is not None:\n",
        "    # Compute mean squared error if everything checks out\n",
        "    mse_best = mean_squared_error(y_test, y_pred_best)\n",
        "    print(f\"Mean Squared Error: {mse_best}\")\n",
        "else:\n",
        "    print(\"Cannot compute mean squared error: y_test or y_pred_best is None\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2LY5ydkcglrx",
        "outputId": "cc6a1b88-63e9-4726-b808-d6c2842edef6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n",
            "True\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'NoneType'>\n",
            "(20,)\n",
            "y_pred_best is None\n",
            "Cannot compute mean squared error: y_test or y_pred_best is None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. What is a weak learner in Gradient Boosting?\n",
        "\n",
        "ANS- A weak learner in the context of gradient boosting refers to a base or individual model within the boosting ensemble that performs slightly better than random chance on a classification or regression problem. These weak learners are often decision trees with limited depth (shallow trees) or nodes, linear regression models, or other simple models.\n",
        "\n",
        "The key characteristic of a weak learner is that it's only slightly better than random guessing on the training data. In boosting, multiple iterations are employed where each subsequent weak learner is focused on learning from the mistakes/errors of its predecessor, thereby improving the overall model's predictive performance. The weak learners are combined sequentially or in parallel to form a strong predictive model through weighted or majority voting schemes.\n",
        "\n",
        "Gradient boosting, specifically, aims to minimize the errors made by the previous weak learners by fitting new models to the residuals or errors left by the previous models. This iterative process allows the ensemble to gradually improve and converge towards a strong predictive model, often with high accuracy."
      ],
      "metadata": {
        "id": "TQklw2SnhX85"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. What is the intuition behind the Gradient Boosting algorithm?\n",
        "\n",
        "ANS- The intuition behind the Gradient Boosting algorithm lies in the idea of sequentially improving the predictive ability of a model by focusing on the errors or residuals made by the previous models in the ensemble.\n",
        "\n",
        "Here's an intuitive breakdown:\n",
        "\n",
        "1. **Starting Point**: Gradient Boosting begins with a simple, weak learner as the initial model, typically a decision tree with limited depth.\n",
        "\n",
        "2. **Sequential Improvement**: It then iteratively builds additional models, where each subsequent model aims to correct the errors or residuals made by the combination of all the preceding models.\n",
        "\n",
        "3. **Learning from Mistakes**: The new models focus on learning the patterns in the residuals left by the previous models. They are built to reduce the errors that the previous models couldn't capture accurately.\n",
        "\n",
        "4. **Gradient Descent Optimization**: The \"gradient\" in Gradient Boosting refers to the technique of minimizing the loss function (like mean squared error for regression or log loss for classification) by moving in the direction that minimizes the loss. It uses gradient descent optimization to find the best parameters for each new model in the ensemble.\n",
        "\n",
        "5. **Combining Weak Learners**: The predictions from all the weak learners (often decision trees) are then combined through a weighted sum or a voting mechanism, giving higher emphasis to those models that perform better on the errors made previously.\n",
        "\n",
        "6. **Creation of Strong Learner**: As this process continues iteratively, the ensemble gradually builds a strong learner that performs significantly better than any individual weak learner.\n",
        "\n",
        "In summary, Gradient Boosting creates a powerful predictive model by iteratively improving upon the weaknesses of its predecessors, focusing on the errors made by previous models, and combining these weak models in an adaptive manner to form a robust and accurate prediction."
      ],
      "metadata": {
        "id": "gwvgaHHEhiAh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n",
        "\n",
        "ANS- The Gradient Boosting algorithm builds an ensemble of weak learners sequentially through an iterative process. Here's a step-by-step explanation of how it constructs this ensemble:\n",
        "\n",
        "1. **Initialization**: The process begins by initializing the ensemble with a simple model, often a decision tree with limited depth, referred to as the base learner or weak learner. This first model makes predictions based on the given features.\n",
        "\n",
        "2. **Residual Calculation**: Next, the algorithm computes the difference (residuals or errors) between the actual target values and the predictions made by the initial weak learner. These residuals represent the errors that the first model couldnâ€™t accurately capture.\n",
        "\n",
        "3. **Fitting Additional Models**: Subsequent weak learners are then trained to fit these residuals or errors. They aim to predict these errors more accurately than the previous model. Each new weak learner focuses on learning the patterns in these residuals.\n",
        "\n",
        "4. **Sequential Learning**: The new model is added to the ensemble, and the predictions from all the weak learners are combined. The combined predictions are used to update the overall model, giving more weight or importance to the areas where the previous models made errors.\n",
        "\n",
        "5. **Iterative Improvement**: This process iterates; at each step, the algorithm fits a new weak learner to the residuals left by the combined predictions of the previous models. The models are added sequentially, and the ensemble learns to progressively reduce the errors made by the previous models.\n",
        "\n",
        "6. **Weighting and Combining Predictions**: Each new model added to the ensemble is assigned a weight based on its performance in reducing the overall error. Predictions from all the weak learners are combined through weighted averaging or voting mechanisms to produce the final prediction.\n",
        "\n",
        "7. **Stopping Criterion**: The process continues for a specified number of iterations or until a predefined stopping criterion (like reaching a certain level of accuracy or when adding more models doesn't significantly reduce errors) is met.\n",
        "\n",
        "Through this iterative process of sequentially fitting new models to the errors or residuals of the previous ones, Gradient Boosting gradually builds an ensemble of weak learners, creating a strong predictive model that combines the strengths of multiple simpler models."
      ],
      "metadata": {
        "id": "dXBVlL81hsEw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting\n",
        "algorithm?\n",
        "\n",
        "ANS- Constructing the mathematical intuition behind the Gradient Boosting algorithm involves understanding the key concepts and steps involved in its implementation. Here's a breakdown of the mathematical intuition behind Gradient Boosting:\n",
        "\n",
        "1. **Objective Function**:\n",
        "   - The algorithm aims to minimize a loss or objective function. For regression problems, this could be Mean Squared Error (MSE), while for classification, it could be Log Loss or another appropriate metric.\n",
        "   - The objective function quantifies the difference between the actual target values and the model's predictions.\n",
        "\n",
        "2. **Initialization**:\n",
        "   - Start with an initial model, often a simple one like a decision tree with limited depth, which serves as the base learner.\n",
        "\n",
        "3. **Residual Calculation**:\n",
        "   - Compute the residuals or errors by taking the difference between the actual target values and the predictions made by the current model. These residuals represent the mistakes the model couldn't capture accurately.\n",
        "\n",
        "4. **Gradient Descent Optimization**:\n",
        "   - Fit a new weak learner (like a decision tree) to the residuals by optimizing the loss function. Gradient descent optimization is used to minimize this loss function.\n",
        "   - The new model learns to predict the residuals left by the combined predictions of the previous models.\n",
        "\n",
        "5. **Weighted Model Addition**:\n",
        "   - Add the new weak learner to the ensemble, with a weight assigned based on its performance in reducing the overall error. The weight signifies the contribution of that model to the final prediction.\n",
        "\n",
        "6. **Updating Predictions**:\n",
        "   - Combine predictions from all weak learners in the ensemble. The combined predictions are used to update the model, focusing more on areas where previous models made errors.\n",
        "\n",
        "7. **Iteration**:\n",
        "   - Repeat steps 3 to 6 iteratively. At each step, a new weak learner is trained to fit the residuals left by the combined predictions of the previous models. The ensemble gradually improves by minimizing the errors iteratively.\n",
        "\n",
        "8. **Stopping Criteria**:\n",
        "   - The process continues for a specified number of iterations or until a stopping criterion is met (e.g., achieving a certain level of accuracy, reaching a maximum number of models, or when further iterations do not significantly reduce errors).\n",
        "\n",
        "9. **Final Prediction**:\n",
        "   - Finally, predictions from all weak learners are combined through weighted averaging or voting to produce the final prediction of the Gradient Boosting model.\n",
        "\n",
        "This mathematical intuition showcases how Gradient Boosting minimizes errors by iteratively improving weak learners, focusing on residual errors, and combining them into a strong predictive model. The iterative nature and sequential improvement on errors form the crux of its mathematical foundation."
      ],
      "metadata": {
        "id": "w14XGbJwh2g8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FBcjTFtigmRX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}